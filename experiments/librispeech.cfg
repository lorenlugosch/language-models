[experiment]
seed=0
folder=experiments/librispeech

[model]
num_tokens=100
num_decoder_layers=3
num_decoder_hidden=512
num_mel_bins=80
tokenizer_training_text_path=librispeech-lm-norm.txt

[training]
base_path=/home/lugosch/data/LibriSpeech
lr=0.00025
lr_period=1
gamma=0.5
batch_size=256
num_epochs=10
use_label_smoothing=False

[inference]
beam_width=1
